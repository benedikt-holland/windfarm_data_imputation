{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2df7928e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import math\n",
    "import torch_geometric as pyg\n",
    "from torch_geometric import EdgeIndex\n",
    "import random\n",
    "import networkx as nx\n",
    "import heapq\n",
    "from tqdm import tqdm\n",
    "from enum import Enum\n",
    "import functools\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "torch.set_default_device(device)\n",
    "torch.set_default_dtype(torch.float32)\n",
    "\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "860d0172",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = pd.read_csv(\"../data/processed_data.csv\")\n",
    "position_df = pd.read_csv(\"../data/turbines.csv\")\n",
    "\n",
    "# Zero-indexed makes indexing easier\n",
    "data_df[\"TurbID\"] -= 1\n",
    "position_df[\"TurbID\"] -= 1\n",
    "data_df[\"Day\"] -= 1\n",
    "\n",
    "# Number of rows in data\n",
    "R = len(data_df)\n",
    "\n",
    "# Number of turbines\n",
    "N = len(position_df)\n",
    "\n",
    "# For ease-of-access, normalize turbine coordinates to [0,1] and put them in a dictionary\n",
    "positions_by_id = {}\n",
    "minX = position_df[\"x\"].min()\n",
    "maxX = position_df[\"x\"].max()\n",
    "minY = position_df[\"y\"].min()\n",
    "maxY = position_df[\"y\"].max()\n",
    "for _, row in position_df.iterrows():\n",
    "    norm_x = (row[\"x\"] - minX) / (maxX - minX)\n",
    "    norm_y = (row[\"y\"] - minY) / (maxY - minY)\n",
    "    positions_by_id[row[\"TurbID\"].astype(int)] = (norm_x, norm_y)\n",
    "\n",
    "# Calculate timestamps in the range [0,T) where T is the total number of unique timestamps\n",
    "time_minutes = data_df[\"Tmstamp\"].str.split(\":\", expand=True).astype(int)\n",
    "total_minutes_of_day = time_minutes[0] * 60 + time_minutes[1]\n",
    "integral_timestamps = (data_df[\"Day\"] * 144 + (total_minutes_of_day // 10)).to_numpy(dtype=int)\n",
    "T = integral_timestamps.max() + 1\n",
    "\n",
    "# Discard useless columns, extract features from dataframe, normalize all features to [0,1]\n",
    "data_df.drop(columns=[\"Day\", \"Tmstamp\", \"datetime\", \"P_norm\"], inplace=True)\n",
    "nodes = np.array(data_df[\"TurbID\"], dtype=int)\n",
    "vals = data_df.values.astype(np.float32)\n",
    "mins = np.array(list(data_df.min(axis=0, skipna=True))).reshape((1, -1))\n",
    "maxs = np.array(data_df.max(axis=0, skipna=True)).reshape((1, -1))\n",
    "vals = (vals - mins) / (maxs - mins)\n",
    "\n",
    "# F is the number of features (10)\n",
    "# Features: [Wspd, Wdir, Etmp, Itmp, Ndir, Pab1, Pab2, Pab3, Prtv, Patv[] (normalized to [0,1]!)\n",
    "F = vals.shape[1]-1\n",
    "\n",
    "# Construct data tensor, replace NaNs with -100\n",
    "X = torch.zeros((T, N, F), dtype=torch.float32, device=device, requires_grad=False)\n",
    "display(data_df)\n",
    "X[integral_timestamps, nodes] = torch.tensor(np.nan_to_num(vals[:,1:], nan=-100), dtype=torch.float32, requires_grad=False)\n",
    "\n",
    "# Data tensor has shape (T,N,F)\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be25884f",
   "metadata": {},
   "outputs": [],
   "source": [
    "SZ = 4\n",
    "L = 1\n",
    "while SZ // 2 < T:\n",
    "    SZ *= 2\n",
    "    L += 1\n",
    "\n",
    "ST = SZ // 2\n",
    "LEVEL = torch.zeros((SZ,), dtype=torch.int32, device='cpu')\n",
    "Z = None\n",
    "CNT = None\n",
    "\n",
    "def get_Z(mask_fn):\n",
    "    global Z, CNT\n",
    "    Z = torch.zeros((SZ,X.shape[1],X.shape[2]), dtype=torch.float32)\n",
    "    Z[ST:ST+T] = X\n",
    "    # print(torch.sum(PAR == -1))\n",
    "\n",
    "    # Random experiment\n",
    "    mask_fn(Z)\n",
    "    # ratio = 0.05\n",
    "    # size = int(T * N * ratio)\n",
    "    # t_mask = np.random.randint(ST, ST+T, size=size)\n",
    "    # n_mask = np.random.randint(0, N, size=size)\n",
    "    # Z[t_mask,n_mask,-1] = -100\n",
    "\n",
    "    CNT = torch.zeros((SZ,N), dtype=torch.int32)\n",
    "    CNT[ST:ST+T] = Z[ST:ST+T,:,-1] > -90\n",
    "\n",
    "    for i in range(ST-1, 0, -1):\n",
    "        CNT[i] = CNT[i*2] + CNT[i*2+1]\n",
    "        LEVEL[i] = LEVEL[i*2] + 1\n",
    "        idx = CNT[i] > 0\n",
    "        Z[i,idx] = (CNT[i*2,idx].view(-1, 1) * Z[i*2,idx] + CNT[i*2+1,idx].view(-1, 1) * Z[i*2+1,idx]) / CNT[i,idx].view(-1, 1)\n",
    "\n",
    "# del X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0625fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "\n",
    "    def __init__(self, in_features, hidden1, hidden2, out_features, sp=False):\n",
    "        super(Model, self).__init__()\n",
    "        self.use_sp = sp\n",
    "        self.seq = nn.Sequential(\n",
    "            nn.Linear(in_features, hidden1),\n",
    "            nn.SELU(),\n",
    "            nn.Linear(hidden1, hidden2),\n",
    "            nn.SELU(),\n",
    "            nn.Linear(hidden2, out_features)\n",
    "        )\n",
    "        self.sp = nn.Softplus()\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        x = self.seq(x)\n",
    "        # if self.use_sp:\n",
    "        #     x = self.sp(x) + 1e-5\n",
    "        return x\n",
    "    \n",
    "def features(chosen_src, chosen_tgt, target_time, source_time, node_src, node_tgt, level):\n",
    "    src_x, src_y = positions_by_id[node_src]\n",
    "    tgt_x, tgt_y = positions_by_id[node_tgt]\n",
    "    point = torch.concat([\n",
    "        torch.tensor([\n",
    "            target_time - source_time, \n",
    "            tgt_x - src_x, \n",
    "            tgt_y - src_y,\n",
    "            level / L,\n",
    "        ], dtype=torch.float32), # metadata\n",
    "        chosen_src, # all information about source\n",
    "        chosen_tgt[9:], # label of dest (not passed into neural networks)\n",
    "    ])\n",
    "    return point\n",
    "    \n",
    "def test_batch(batch_size: int, sampling_stddev = 2, repeats=3):\n",
    "    \n",
    "    T_target = np.repeat(np.random.choice(np.arange(0, T), batch_size, replace=False), repeats=repeats)\n",
    "\n",
    "    parts = []\n",
    "    for target_time in T_target:\n",
    "        source_time = np.clip(np.rint(np.random.normal(target_time, sampling_stddev)).astype(int), 0, T-1)\n",
    "\n",
    "        # source_level = random.randint(0, L)\n",
    "        # source_level = 0\n",
    "        source_level = np.minimum(np.rint(np.random.exponential(scale=1/3)).astype(int), L)\n",
    "        st = source_time + ST\n",
    "        for _ in range(source_level):\n",
    "            st //= 2\n",
    "        chosen_src = None\n",
    "        node_src = None\n",
    "        while chosen_src is None or chosen_src[-1] < -90:\n",
    "            node_src = random.randint(0, N-1)\n",
    "            chosen_src = Z[st][node_src]\n",
    "\n",
    "        chosen_tgt = None\n",
    "        node_tgt = None\n",
    "        while chosen_tgt is None or chosen_tgt[-1] < -90 or (source_time == target_time and node_src == node_tgt):\n",
    "            node_tgt = random.randint(0, N-1)\n",
    "            chosen_tgt = Z[target_time + ST][node_tgt]\n",
    "        point = features(chosen_src, chosen_tgt, target_time, source_time, node_src, node_tgt, source_level)\n",
    "        parts.append(point)\n",
    "    return torch.stack(parts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aff72ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model_pred, optimizer, epochs = 100, batch_size = 256, batch_count = 20, repeats=3, sampling_stddev=5, quiet = True):\n",
    "    criterion = nn.MSELoss()\n",
    "    print(f\"Model loaded on {device} for training.\") # just for debugging\n",
    "    acc = 0\n",
    "    model_pred.train()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for batch_idx in range(batch_count):\n",
    "            batch = test_batch(batch_size, repeats=repeats, sampling_stddev=sampling_stddev)\n",
    "            batch_X = batch[:,:-1]\n",
    "            batch_y = batch[:,-1]\n",
    "            optimizer.zero_grad()\n",
    "            y_pred = model_pred(batch_X).squeeze(-1)\n",
    "            loss_prediction = criterion(y_pred, batch_y)\n",
    "            loss_prediction.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss_prediction.item()\n",
    "            if not quiet and batch_idx == batch_count-1:\n",
    "                print(\" ### \")\n",
    "                print(\"Predictions: \", list(y_pred[:5].detach().cpu().numpy()))\n",
    "                print(\"Trues: \", list(batch_y[:5].detach().cpu().numpy()))\n",
    "\n",
    "        if not quiet:\n",
    "            print(f'Epoch {epoch}, Loss: {total_loss:.4f}')\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f848c9a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tb = test_batch(10)\n",
    "# in_features = tb.shape[1]-1\n",
    "# model_pred = Model(in_features, 128, 64, 1).to(torch.float32)\n",
    "# # model_weight = Model(in_features, 128, 64, 1, sp=True)\n",
    "# optimizer_pred = optim.Adam(model_pred.parameters(), lr=1e-3)\n",
    "# # optimizer_weight = optim.Adam(model_weight.parameters(), lr=1e-3)\n",
    "# train(model_pred, optimizer_pred, epochs=20, repeats=1, sampling_stddev=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36585e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(model_pred.state_dict(), \"prediction_model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9728b5ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train2(model_pred, model_weight, optimizer, epochs = 100, batch_size = 256, batch_count = 20, repeats=3, sampling_stddev=2, quiet = True):\n",
    "    criterion = nn.MSELoss()\n",
    "    print(f\"Model loaded on {device} for training.\") # just for debugging\n",
    "    acc = 0\n",
    "    model_pred.eval()\n",
    "    model_weight.train()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for batch_idx in range(batch_count):\n",
    "            batch = test_batch(batch_size, repeats=repeats, sampling_stddev=sampling_stddev)\n",
    "            batch_X = batch[:,:-1]\n",
    "            batch_y = batch[:,-1]\n",
    "            optimizer.zero_grad()\n",
    "            y_weight = model_weight(batch_X)\n",
    "            y_pred = model_pred(batch_X).squeeze(-1)\n",
    "            pred_resized = y_pred.resize(batch_size, repeats)\n",
    "            pred_weight = torch.clamp(y_weight.resize(batch_size, repeats), min=1e-5)\n",
    "            y_pred_repeats = (torch.sum(pred_weight * pred_resized, dim=1) / torch.sum(pred_weight, dim=1))\n",
    "            loss_weight = criterion(y_pred_repeats, batch_y[::repeats]) + criterion(y_weight, torch.clamp(y_weight, -1, 1))\n",
    "            # loss_weight -= 0.1 * torch.std(y_weight)\n",
    "            loss_weight.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss_weight.item()\n",
    "            if not quiet and batch_idx == batch_count-1:\n",
    "                print(\" ### \")\n",
    "                print(\"Predictions: \", list(y_pred[:5].detach().cpu().numpy()))\n",
    "                print(\"Weighted: \", list(y_pred_repeats[:5].detach().cpu().numpy()))\n",
    "                print(\"Trues: \", list(batch_y[:5].detach().cpu().numpy()))\n",
    "                print(\"Weights: \", list(y_weight[:5,0].detach().cpu().numpy()))\n",
    "\n",
    "        if not quiet:\n",
    "            print(f'Epoch {epoch}, Loss: {total_loss:.4f}')\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31471fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tb = test_batch(10)\n",
    "# in_features = tb.shape[1]-1\n",
    "# model_weight = Model(in_features, 128, 64, 1)\n",
    "# # model_weight = Model(in_features, 128, 64, 1, sp=True)\n",
    "# optimizer_weight = optim.Adam(model_weight.parameters(), lr=1e-3)\n",
    "# # optimizer_weight = optim.Adam(model_weight.parameters(), lr=1e-3)\n",
    "# train2(model_pred, model_weight, optimizer_weight, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "324c915f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(model_pred.state_dict(), \"prediction_model.pth\")\n",
    "# torch.save(model_weight.state_dict(), \"weight_model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edbde66a",
   "metadata": {},
   "outputs": [],
   "source": [
    "M = len(positions_by_id)\n",
    "\n",
    "def is_north(cand, origin):\n",
    "    cand_x, cand_y = cand\n",
    "    origin_x, origin_y = origin\n",
    "    dx = abs(cand_x - origin_x)\n",
    "    dy = abs(cand_y - origin_y)\n",
    "    return cand_y > origin_y and dy > dx\n",
    "\n",
    "def is_west(cand, origin):\n",
    "    cand_x, cand_y = cand\n",
    "    origin_x, origin_y = origin\n",
    "    dx = abs(cand_x - origin_x)\n",
    "    dy = abs(cand_y - origin_y)\n",
    "    return cand_x < origin_x and dx > dy\n",
    "\n",
    "def is_east(cand, origin):\n",
    "    cand_x, cand_y = cand\n",
    "    origin_x, origin_y = origin\n",
    "    dx = abs(cand_x - origin_x)\n",
    "    dy = abs(cand_y - origin_y)\n",
    "    return cand_x > origin_x and dx > dy\n",
    "\n",
    "def is_south(cand, origin):\n",
    "    cand_x, cand_y = cand\n",
    "    origin_x, origin_y = origin\n",
    "    dx = abs(cand_x - origin_x)\n",
    "    dy = abs(cand_y - origin_y)\n",
    "    return cand_y < origin_y and dy > dx\n",
    "\n",
    "def find_best(origin_id, origin, filter):\n",
    "    best_id = None\n",
    "    best_dist = 1e18\n",
    "    for id, cand in positions_by_id.items():\n",
    "        dist = math.hypot(origin[0] - cand[0], origin[1] - cand[1])\n",
    "        if id != origin_id and filter(cand, origin) and dist < best_dist:\n",
    "            best_dist = dist\n",
    "            best_id = id\n",
    "    return best_id\n",
    "\n",
    "G = nx.Graph()\n",
    "for id, origin in positions_by_id.items():\n",
    "    for filter in [is_north, is_west, is_east, is_south]:\n",
    "        best_id = find_best(id, origin, filter)\n",
    "        if best_id is not None:\n",
    "            G.add_edge(id, best_id)\n",
    "\n",
    "print(G.number_of_edges())\n",
    "labels = {}\n",
    "for id, pos in positions_by_id.items():\n",
    "    labels[id] = f\"{id+1}\"\n",
    "nx.draw(G, pos=positions_by_id, labels=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da7386f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Imputer:\n",
    "\n",
    "    vis_limit: int\n",
    "    vis_count: int\n",
    "\n",
    "    def __init__(self, model_pred, model_weight, vis_limit=10):\n",
    "        self.vis_limit = vis_limit\n",
    "        self.model_pred = model_pred\n",
    "        self.model_weight = model_weight\n",
    "\n",
    "    def expand_dir(self, tm, tm_dt, neighbor, target, visited, queue, tgt, level):\n",
    "        key = (tm_dt, level, neighbor)\n",
    "        z_src = tm_dt + ST\n",
    "        for _ in range(level):\n",
    "            z_src //= 2\n",
    "        if CNT[z_src][neighbor] > 0 and Z[z_src][neighbor][-1] > -90 and key not in visited:\n",
    "            visited.add(key)\n",
    "            feature_v = features(Z[z_src][neighbor], target, tm, tm_dt, neighbor, tgt, level)\n",
    "            X_chosen = feature_v[:-1]\n",
    "            w = self.model_weight(X_chosen).item()\n",
    "            v = self.model_pred(X_chosen).item()\n",
    "            if self.vis_count < self.vis_limit and w > 0:\n",
    "                self.vis_count += 1\n",
    "                heapq.heappush(queue, (-w,v,tm_dt,level,neighbor))\n",
    "\n",
    "    def expand(self, tm, node, target, visited, queue, level):\n",
    "        for dt in [-1, 1]:\n",
    "            tm_dt = tm + dt\n",
    "            if tm_dt >= ST and tm_dt < SZ:\n",
    "                self.expand_dir(tm, tm_dt, node, target, visited, queue, node, level)\n",
    "        if level < L:\n",
    "            self.expand_dir(tm, tm, node, target, visited, queue, node, level+1)\n",
    "        for neighbor in G.adj[node]:\n",
    "            self.expand_dir(tm, tm, neighbor, target, visited, queue, node, level)\n",
    "\n",
    "    def predict(self, tm, node):\n",
    "        self.vis_count = 0\n",
    "        visited = set()\n",
    "        visited.add((tm, 0, node))\n",
    "        target = Z[tm][node]\n",
    "        queue = []\n",
    "        self.expand(tm, node, target, visited, queue, 0)\n",
    "\n",
    "        parts = []\n",
    "        while len(queue) > 0:\n",
    "            neg_w, v, tm_loc, lev_loc, node_loc = heapq.heappop(queue)\n",
    "            w = -neg_w\n",
    "            parts.append([w,v])\n",
    "            self.expand(tm_loc, node_loc, target, visited, queue, lev_loc)\n",
    "        arr = np.array(parts)\n",
    "        if len(arr) == 0:\n",
    "            print(\"oops\")\n",
    "            return 0\n",
    "        pred = np.sum(arr[:,0] * arr[:,1]) / np.sum(arr[:,0])\n",
    "        return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3616738",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_mask = None\n",
    "n_mask = None\n",
    "\n",
    "def mask_random(Z, ratio = 0.01):\n",
    "    global t_mask, n_mask\n",
    "    size = int(T * N * ratio)\n",
    "    t_mask = np.random.randint(ST, ST+T, size=size)\n",
    "    n_mask = np.random.randint(0, N, size=size)\n",
    "    Z[t_mask,n_mask,-1] = -100\n",
    "\n",
    "def mask_blackout(Z, minutes=30, ratio=0.01):\n",
    "    global t_mask, n_mask\n",
    "    size = int(T * N * ratio)\n",
    "    cnt = minutes // 10\n",
    "    n = size // cnt\n",
    "    placed = torch.zeros((SZ, N), dtype=torch.bool, device='cpu')\n",
    "    t_mask = np.zeros((n*cnt), dtype=int)\n",
    "    n_mask = np.zeros((n*cnt), dtype=int)\n",
    "    i = 0\n",
    "    while n > 0:\n",
    "        turbine = random.randint(0,N-1)\n",
    "        t = random.randint(ST, T+ST-cnt-1)\n",
    "        if torch.sum(placed[t:t+cnt,turbine]) > 0:\n",
    "            continue\n",
    "        Z[t:t+cnt,turbine,-1] = -100\n",
    "        placed[t:t+cnt+cnt//2,turbine] = True\n",
    "        t_mask[i:i+cnt] = t\n",
    "        n_mask[i:i+cnt] = turbine\n",
    "        n -= 1\n",
    "        i += cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef89e2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error, root_mean_squared_error, mean_squared_error, median_absolute_error\n",
    "import time\n",
    "\n",
    "class Experiment(Enum):\n",
    "    RANDOM = \"random\"\n",
    "    BLACKOUT = \"blackout\"\n",
    "    MAINTENANCE = \"maintenance\"\n",
    "\n",
    "ratio = 0.01\n",
    "experiments = [\n",
    "    # (Experiment.RANDOM, None),\n",
    "    # (Experiment.BLACKOUT, 30),\n",
    "    # (Experiment.BLACKOUT, 60),\n",
    "    # (Experiment.BLACKOUT, 150),\n",
    "    # (Experiment.BLACKOUT, 300),\n",
    "    # (Experiment.MAINTENANCE, 1),\n",
    "    (Experiment.MAINTENANCE, 2),\n",
    "    (Experiment.MAINTENANCE, 7),\n",
    "    (Experiment.MAINTENANCE, 14),\n",
    "]\n",
    "\n",
    "results = []\n",
    "for experiment, size in experiments:\n",
    "    print(f\"Running experiment {experiment} with size {size}\")\n",
    "\n",
    "    mask_fn = None\n",
    "    if experiment == Experiment.RANDOM:\n",
    "        mask_fn = functools.partial(mask_random, ratio=ratio)\n",
    "    if experiment == Experiment.BLACKOUT:\n",
    "        mask_fn = functools.partial(mask_blackout, ratio=ratio, minutes=size)\n",
    "    if experiment == Experiment.MAINTENANCE:\n",
    "        mask_fn = functools.partial(mask_blackout, ratio=ratio, minutes=size*60*24)\n",
    "    \n",
    "    get_Z(mask_fn)\n",
    "    print(\"Generated masked data\")\n",
    "\n",
    "    tb = test_batch(10)\n",
    "    in_features = tb.shape[1]-1\n",
    "    model_pred = Model(in_features, 128, 64, 1)\n",
    "    train_start = time.time()\n",
    "    optimizer_pred = optim.Adam(model_pred.parameters(), lr=1e-3)\n",
    "    train(model_pred, optimizer_pred, epochs=20, repeats=1, sampling_stddev=5)\n",
    "    model_pred.eval()\n",
    "    print(\"Trained prediction model\")\n",
    "\n",
    "    in_features = tb.shape[1]-1\n",
    "    model_weight = Model(in_features, 128, 64, 1)\n",
    "    optimizer_weight = optim.Adam(model_weight.parameters(), lr=1e-3)\n",
    "    train2(model_pred, model_weight, optimizer_weight, epochs=10)\n",
    "    train_time = time.time() - train_start\n",
    "    model_weight.eval()\n",
    "    print(f\"All training complete in {train_time} seconds\")\n",
    "\n",
    "    imputer = Imputer(model_pred, model_weight, vis_limit=10)\n",
    "    valids = X[t_mask-ST,n_mask,-1] > -90\n",
    "    tmsk = t_mask[valids.cpu().numpy()]\n",
    "    nmsk = n_mask[valids.cpu().numpy()]\n",
    "    preds = []\n",
    "    trues = []\n",
    "\n",
    "    limit = 1000\n",
    "    eval_start = time.time()\n",
    "    eval_idxs = np.random.choice(np.arange(0, T), size=limit, replace=False)\n",
    "    for i, (t, n) in tqdm(enumerate(zip(tmsk[eval_idxs], nmsk[eval_idxs]))):\n",
    "        preds.append(imputer.predict(t-ST, n))\n",
    "        trues.append(X[t-ST, n, -1].cpu().numpy())\n",
    "    eval_time = time.time() - eval_start\n",
    "    print(f\"Time to impute {limit} distinct values: {eval_time}\")\n",
    "\n",
    "    preds = np.array(preds)\n",
    "    trues = np.array(trues)\n",
    "    mse = mean_squared_error(trues, preds)\n",
    "    mae = mean_absolute_error(trues, preds)\n",
    "    rmse = root_mean_squared_error(trues, preds)\n",
    "    medae = median_absolute_error(trues, preds)\n",
    "\n",
    "    print(f\"MSE: {mse}, MAE: {mae}, RMSE: {rmse}, MEDAE: {medae}\")\n",
    "    results.append({\n",
    "        \"experiment\": experiment,\n",
    "        \"size\": size,\n",
    "        \"mse\": mse,\n",
    "        \"mae\": mae,\n",
    "        \"rmse\": rmse,\n",
    "        \"medae\": medae,\n",
    "        \"train_time\": train_time,\n",
    "        \"eval_time\": eval_time,\n",
    "    })\n",
    "    df = pd.DataFrame(data=results)\n",
    "    df.to_csv(\"main_method_results.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
