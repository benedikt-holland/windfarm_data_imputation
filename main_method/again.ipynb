{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2df7928e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import math\n",
    "import torch_geometric as pyg\n",
    "from torch_geometric import EdgeIndex\n",
    "import random\n",
    "import networkx as nx\n",
    "import heapq\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "torch.set_default_device(device)\n",
    "torch.set_default_dtype(torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "860d0172",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = pd.read_csv(\"processed_data.csv\")\n",
    "position_df = pd.read_csv(\"sdwpf_baidukddcup2022_turb_location.CSV\")\n",
    "\n",
    "# Zero-indexed makes indexing easier\n",
    "data_df[\"TurbID\"] -= 1\n",
    "position_df[\"TurbID\"] -= 1\n",
    "data_df[\"Day\"] -= 1\n",
    "\n",
    "# Number of rows in data\n",
    "R = len(data_df)\n",
    "\n",
    "# Number of turbines\n",
    "N = len(position_df)\n",
    "\n",
    "# For ease-of-access, normalize turbine coordinates to [0,1] and put them in a dictionary\n",
    "positions_by_id = {}\n",
    "minX = position_df[\"x\"].min()\n",
    "maxX = position_df[\"x\"].max()\n",
    "minY = position_df[\"y\"].min()\n",
    "maxY = position_df[\"y\"].max()\n",
    "for _, row in position_df.iterrows():\n",
    "    norm_x = (row[\"x\"] - minX) / (maxX - minX)\n",
    "    norm_y = (row[\"y\"] - minY) / (maxY - minY)\n",
    "    positions_by_id[row[\"TurbID\"].astype(int)] = (norm_x, norm_y)\n",
    "\n",
    "# Calculate timestamps in the range [0,T) where T is the total number of unique timestamps\n",
    "time_minutes = data_df[\"Tmstamp\"].str.split(\":\", expand=True).astype(int)\n",
    "total_minutes_of_day = time_minutes[0] * 60 + time_minutes[1]\n",
    "integral_timestamps = (data_df[\"Day\"] * 144 + (total_minutes_of_day // 10)).to_numpy(dtype=int)\n",
    "T = integral_timestamps.max() + 1\n",
    "\n",
    "# Discard useless columns, extract features from dataframe, normalize all features to [0,1]\n",
    "data_df.drop(columns=[\"Day\", \"Tmstamp\", \"datetime\", \"P_norm\"], inplace=True)\n",
    "nodes = np.array(data_df[\"TurbID\"], dtype=int)\n",
    "vals = data_df.values.astype(np.float32)\n",
    "mins = np.array(list(data_df.min(axis=0, skipna=True))).reshape((1, -1))\n",
    "maxs = np.array(data_df.max(axis=0, skipna=True)).reshape((1, -1))\n",
    "vals = (vals - mins) / (maxs - mins)\n",
    "\n",
    "# F is the number of features (10)\n",
    "# Features: [Wspd, Wdir, Etmp, Itmp, Ndir, Pab1, Pab2, Pab3, Prtv, Patv[] (normalized to [0,1]!)\n",
    "F = vals.shape[1]-1\n",
    "\n",
    "# Construct data tensor, replace NaNs with -100\n",
    "X = torch.zeros((T, N, F), dtype=torch.float32, device=device, requires_grad=False)\n",
    "display(data_df)\n",
    "X[integral_timestamps, nodes] = torch.tensor(np.nan_to_num(vals[:,1:], nan=-100), dtype=torch.float32, requires_grad=False)\n",
    "\n",
    "# Data tensor has shape (T,N,F)\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be25884f",
   "metadata": {},
   "outputs": [],
   "source": [
    "SZ = 4\n",
    "L = 1\n",
    "while SZ // 2 < T:\n",
    "    SZ *= 2\n",
    "    L += 1\n",
    "\n",
    "ST = SZ // 2\n",
    "CNT = torch.zeros((SZ,N), dtype=torch.int32)\n",
    "LEVEL = torch.zeros((SZ,), dtype=torch.int32, device='cpu')\n",
    "CNT[ST:ST+T] = X[:,:,-1] > -90\n",
    "Z = torch.zeros((SZ,X.shape[1],X.shape[2]), dtype=torch.float32)\n",
    "Z[ST:ST+T] = X\n",
    "# print(torch.sum(PAR == -1))\n",
    "\n",
    "for i in range(ST-1, 0, -1):\n",
    "    CNT[i] = CNT[i*2] + CNT[i*2+1]\n",
    "    LEVEL[i] = LEVEL[i*2] + 1\n",
    "    idx = CNT[i] > 0\n",
    "    Z[i,idx] = (CNT[i*2,idx].view(-1, 1) * Z[i*2,idx] + CNT[i*2+1,idx].view(-1, 1) * Z[i*2+1,idx]) / CNT[i,idx].view(-1, 1)\n",
    "\n",
    "# del X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0625fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "\n",
    "    def __init__(self, in_features, hidden1, hidden2, out_features, sp=False):\n",
    "        super(Model, self).__init__()\n",
    "        self.use_sp = sp\n",
    "        self.seq = nn.Sequential(\n",
    "            nn.Linear(in_features, hidden1),\n",
    "            nn.SELU(),\n",
    "            nn.Linear(hidden1, hidden2),\n",
    "            nn.SELU(),\n",
    "            nn.Linear(hidden2, out_features)\n",
    "        )\n",
    "        self.sp = nn.Softplus()\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        x = self.seq(x)\n",
    "        # if self.use_sp:\n",
    "        #     x = self.sp(x) + 1e-5\n",
    "        return x\n",
    "    \n",
    "def features(chosen_src, chosen_tgt, target_time, source_time, node_src, node_tgt, level):\n",
    "    src_x, src_y = positions_by_id[node_src]\n",
    "    tgt_x, tgt_y = positions_by_id[node_tgt]\n",
    "    point = torch.concat([\n",
    "        torch.tensor([\n",
    "            target_time - source_time, \n",
    "            tgt_x - src_x, \n",
    "            tgt_y - src_y,\n",
    "            level / L,\n",
    "        ], dtype=torch.float32), # metadata\n",
    "        chosen_src, # all information about source\n",
    "        chosen_tgt[9:], # label of dest (not passed into neural networks)\n",
    "    ])\n",
    "    return point\n",
    "    \n",
    "def test_batch(batch_size: int, sampling_stddev = 2, repeats=3):\n",
    "    \n",
    "    T_target = np.repeat(np.random.choice(np.arange(0, T), batch_size, replace=False), repeats=repeats)\n",
    "\n",
    "    parts = []\n",
    "    for target_time in T_target:\n",
    "        source_time = np.clip(np.rint(np.random.normal(target_time, sampling_stddev)).astype(int), 0, T-1)\n",
    "\n",
    "        # source_level = random.randint(0, L)\n",
    "        # source_level = 0\n",
    "        source_level = np.minimum(np.rint(np.random.exponential(scale=3)).astype(int), L)\n",
    "        st = source_time + ST\n",
    "        for _ in range(source_level):\n",
    "            st //= 2\n",
    "        chosen_src = None\n",
    "        node_src = None\n",
    "        while chosen_src is None or chosen_src[-1] < -90:\n",
    "            node_src = random.randint(0, N-1)\n",
    "            chosen_src = Z[st][node_src]\n",
    "\n",
    "        chosen_tgt = None\n",
    "        node_tgt = None\n",
    "        while chosen_tgt is None or chosen_tgt[-1] < -90:\n",
    "            node_tgt = random.randint(0, N-1)\n",
    "            chosen_tgt = Z[target_time + ST][node_tgt]\n",
    "        point = features(chosen_src, chosen_tgt, target_time, source_time, node_src, node_tgt, source_level)\n",
    "        parts.append(point)\n",
    "    return torch.stack(parts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa9a62c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "u = test_batch(100)\n",
    "print(u.shape, u.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aff72ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model_pred, optimizer, epochs = 100, batch_size = 256, batch_count = 20, repeats=3, sampling_stddev=5):\n",
    "    criterion = nn.MSELoss()\n",
    "    print(f\"Model loaded on {device} for training.\") # just for debugging\n",
    "    acc = 0\n",
    "    model_pred.train()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for batch_idx in range(batch_count):\n",
    "            batch = test_batch(batch_size, repeats=repeats, sampling_stddev=sampling_stddev)\n",
    "            batch_X = batch[:,:-1]\n",
    "            batch_y = batch[:,-1]\n",
    "            optimizer.zero_grad()\n",
    "            y_pred = model_pred(batch_X).squeeze(-1)\n",
    "            loss_prediction = criterion(y_pred, batch_y)\n",
    "            loss_prediction.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss_prediction.item()\n",
    "            if batch_idx == batch_count-1:\n",
    "                print(\" ### \")\n",
    "                print(\"Predictions: \", list(y_pred[:5].detach().cpu().numpy()))\n",
    "                print(\"Trues: \", list(batch_y[:5].detach().cpu().numpy()))\n",
    "\n",
    "        print(f'Epoch {epoch}, Loss: {total_loss:.4f}')\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f848c9a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tb = test_batch(10)\n",
    "in_features = tb.shape[1]-1\n",
    "model_pred = Model(in_features, 128, 64, 1).to(torch.float32)\n",
    "# model_weight = Model(in_features, 128, 64, 1, sp=True)\n",
    "optimizer_pred = optim.Adam(model_pred.parameters(), lr=1e-3)\n",
    "# optimizer_weight = optim.Adam(model_weight.parameters(), lr=1e-3)\n",
    "train(model_pred, optimizer_pred, epochs=20, repeats=1, sampling_stddev=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36585e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(model_pred.state_dict(), \"prediction_model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9728b5ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train2(model_pred, model_weight, optimizer, epochs = 100, batch_size = 256, batch_count = 20, repeats=3, sampling_stddev=2):\n",
    "    criterion = nn.MSELoss()\n",
    "    print(f\"Model loaded on {device} for training.\") # just for debugging\n",
    "    acc = 0\n",
    "    model_pred.eval()\n",
    "    model_weight.train()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for batch_idx in range(batch_count):\n",
    "            batch = test_batch(batch_size, repeats=repeats, sampling_stddev=sampling_stddev)\n",
    "            batch_X = batch[:,:-1]\n",
    "            batch_y = batch[:,-1]\n",
    "            optimizer.zero_grad()\n",
    "            y_weight = model_weight(batch_X)\n",
    "            y_pred = model_pred(batch_X).squeeze(-1)\n",
    "            pred_resized = y_pred.resize(batch_size, repeats)\n",
    "            pred_weight = torch.clamp(y_weight.resize(batch_size, repeats), min=1e-5)\n",
    "            y_pred_repeats = (torch.sum(pred_weight * pred_resized, dim=1) / torch.sum(pred_weight, dim=1))\n",
    "            loss_weight = criterion(y_pred_repeats, batch_y[::repeats]) + criterion(y_weight, torch.clamp(y_weight, -1, 1))\n",
    "            # loss_weight -= 0.1 * torch.std(y_weight)\n",
    "            loss_weight.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss_weight.item()\n",
    "            if batch_idx == batch_count-1:\n",
    "                print(\" ### \")\n",
    "                print(\"Predictions: \", list(y_pred[:5].detach().cpu().numpy()))\n",
    "                print(\"Weighted: \", list(y_pred_repeats[:5].detach().cpu().numpy()))\n",
    "                print(\"Trues: \", list(batch_y[:5].detach().cpu().numpy()))\n",
    "                print(\"Weights: \", list(y_weight[:5,0].detach().cpu().numpy()))\n",
    "\n",
    "        print(f'Epoch {epoch}, Loss: {total_loss:.4f}')\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa84973f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_pred.load_state_dict(torch.load(\"prediction_model.pth\", weights_only=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31471fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tb = test_batch(10)\n",
    "in_features = tb.shape[1]-1\n",
    "model_weight = Model(in_features, 128, 64, 1)\n",
    "# model_weight = Model(in_features, 128, 64, 1, sp=True)\n",
    "optimizer_weight = optim.Adam(model_weight.parameters(), lr=1e-3)\n",
    "# optimizer_weight = optim.Adam(model_weight.parameters(), lr=1e-3)\n",
    "train2(model_pred, model_weight, optimizer_weight, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "324c915f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(model_weight.state_dict(), \"weight_model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edbde66a",
   "metadata": {},
   "outputs": [],
   "source": [
    "M = len(positions_by_id)\n",
    "\n",
    "def is_north(cand, origin):\n",
    "    cand_x, cand_y = cand\n",
    "    origin_x, origin_y = origin\n",
    "    dx = abs(cand_x - origin_x)\n",
    "    dy = abs(cand_y - origin_y)\n",
    "    return cand_y > origin_y and dy > dx\n",
    "\n",
    "def is_west(cand, origin):\n",
    "    cand_x, cand_y = cand\n",
    "    origin_x, origin_y = origin\n",
    "    dx = abs(cand_x - origin_x)\n",
    "    dy = abs(cand_y - origin_y)\n",
    "    return cand_x < origin_x and dx > dy\n",
    "\n",
    "def is_east(cand, origin):\n",
    "    cand_x, cand_y = cand\n",
    "    origin_x, origin_y = origin\n",
    "    dx = abs(cand_x - origin_x)\n",
    "    dy = abs(cand_y - origin_y)\n",
    "    return cand_x > origin_x and dx > dy\n",
    "\n",
    "def is_south(cand, origin):\n",
    "    cand_x, cand_y = cand\n",
    "    origin_x, origin_y = origin\n",
    "    dx = abs(cand_x - origin_x)\n",
    "    dy = abs(cand_y - origin_y)\n",
    "    return cand_y < origin_y and dy > dx\n",
    "\n",
    "def find_best(origin_id, origin, filter):\n",
    "    best_id = None\n",
    "    best_dist = 1e18\n",
    "    for id, cand in positions_by_id.items():\n",
    "        dist = math.hypot(origin[0] - cand[0], origin[1] - cand[1])\n",
    "        if id != origin_id and filter(cand, origin) and dist < best_dist:\n",
    "            best_dist = dist\n",
    "            best_id = id\n",
    "    return best_id\n",
    "\n",
    "G = nx.Graph()\n",
    "for id, origin in positions_by_id.items():\n",
    "    for filter in [is_north, is_west, is_east, is_south]:\n",
    "        best_id = find_best(id, origin, filter)\n",
    "        if best_id is not None:\n",
    "            G.add_edge(id, best_id)\n",
    "\n",
    "print(G.number_of_edges())\n",
    "labels = {}\n",
    "for id, pos in positions_by_id.items():\n",
    "    labels[id] = f\"{id}\"\n",
    "nx.draw(G, pos=positions_by_id, labels=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da7386f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "vis_count = 0\n",
    "\n",
    "def expand_dir(tm, tm_dt, neighbor, target, visited, queue, tgt, level):\n",
    "    global vis_count\n",
    "    key = (tm_dt, level, neighbor)\n",
    "    z_src = tm_dt + ST\n",
    "    for _ in range(level):\n",
    "        z_src //= 2\n",
    "    if CNT[z_src][neighbor] > 0 and Z[z_src][neighbor][-1] > -90 and key not in visited:\n",
    "        visited.add(key)\n",
    "        feature_v = features(Z[z_src][neighbor], target, tm, tm_dt, neighbor, tgt, level)\n",
    "        X_chosen = feature_v[:-1]\n",
    "        w = model_weight(X_chosen).item()\n",
    "        v = model_pred(X_chosen).item()\n",
    "        if vis_count < 50 and w > 0:\n",
    "            vis_count += 1\n",
    "            heapq.heappush(queue, (-w,v,tm_dt,level,neighbor))\n",
    "\n",
    "def expand(tm, node, target, visited, queue, level):\n",
    "    for dt in [-1, 1]:\n",
    "        tm_dt = tm + dt\n",
    "        if tm_dt >= ST and tm_dt < SZ:\n",
    "            expand_dir(tm, tm_dt, node, target, visited, queue, node, level)\n",
    "    if level < L:\n",
    "        expand_dir(tm, tm, node, target, visited, queue, node, level+1)\n",
    "    for neighbor in G.adj[node]:\n",
    "        expand_dir(tm, tm, neighbor, target, visited, queue, node, level)\n",
    "\n",
    "def predict(tm, node):\n",
    "    visited = set()\n",
    "    visited.add((tm, 0, node))\n",
    "    target = Z[tm][node]\n",
    "    queue = []\n",
    "    expand(tm, node, target, visited, queue, 0)\n",
    "\n",
    "    parts = []\n",
    "    while len(queue) > 0:\n",
    "        neg_w, v, tm_loc, lev_loc, node_loc = heapq.heappop(queue)\n",
    "        w = -neg_w\n",
    "        parts.append([w,v])\n",
    "        expand(tm_loc, node_loc, target, visited, queue, lev_loc)\n",
    "    arr = np.array(parts)\n",
    "    pred = np.sum(arr[:,0] * arr[:,1]) / np.sum(arr[:,0])\n",
    "    return pred\n",
    "\n",
    "t = 9000\n",
    "node = 105\n",
    "u = predict(t, node)\n",
    "print(u, X[t][node][-1], vis_count)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
