{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-10-29T15:20:48.522540Z",
     "start_time": "2025-10-29T15:20:48.493034Z"
    }
   },
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import sys\n",
    "\n",
    "current_dir = os.getcwd()\n",
    "root_dir = os.path.join(current_dir, '..')\n",
    "sys.path.insert(0, root_dir)\n",
    "\n",
    "from GCGRU.GRU import GRU\n",
    "\n",
    "torch.manual_seed(10)\n",
    "np.random.seed(10)"
   ],
   "outputs": [],
   "execution_count": 205
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-29T15:21:08.637485Z",
     "start_time": "2025-10-29T15:21:00.444797Z"
    }
   },
   "cell_type": "code",
   "source": [
    "data = pd.read_csv('../processed_data.csv')\n",
    "data = data[data['TurbID'] == 1]['P_norm']"
   ],
   "id": "3f605d11e577adcb",
   "outputs": [],
   "execution_count": 209
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-29T15:21:08.680377Z",
     "start_time": "2025-10-29T15:21:08.674215Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class GRUBaseline(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size=64):\n",
    "        super().__init__()\n",
    "        # Input x2 because of the mask\n",
    "        self.gru = nn.GRU(input_size * 2, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, input_size)\n",
    "\n",
    "    def forward(self, x, mask, h=None):\n",
    "        # Concatenate values and mask as input\n",
    "        x_masked = torch.cat([x, mask], dim=-1)\n",
    "        out, h = self.gru(x_masked, h)\n",
    "        return self.fc(out), h"
   ],
   "id": "d19618abf18cba86",
   "outputs": [],
   "execution_count": 210
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-29T15:21:08.702148Z",
     "start_time": "2025-10-29T15:21:08.689600Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def train_baseline(data, train_input_mask, train_loss_mask, epochs=20, chunk_size=500, lr=0.001):\n",
    "    n_samples, n_timesteps, n_features = data.shape\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "    criterion = nn.MSELoss(reduction='none')\n",
    "    model = GRUBaseline(n_features).to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        count = 0\n",
    "\n",
    "        for i in range(n_samples):\n",
    "            h = None\n",
    "            for start in range(0, n_timesteps, chunk_size):\n",
    "                end = min(start + chunk_size, n_timesteps)\n",
    "\n",
    "                chunk = data[i, start:end].clone().to(device)\n",
    "                input_mask = train_input_mask[i, start:end].to(device)\n",
    "                loss_mask = train_loss_mask[i, start:end-1].to(device)\n",
    "\n",
    "                chunk[input_mask] = -1.0\n",
    "\n",
    "                # Input to model: all but last timestep\n",
    "                x = chunk[:-1].unsqueeze(0)   # [1, chunk_len - 1, n_features]\n",
    "                target = chunk[1:].unsqueeze(0)  # [1, chunk_len - 1, n_features]\n",
    "\n",
    "                input_mask = input_mask[:-1].unsqueeze(0)\n",
    "                loss_mask = loss_mask.unsqueeze(0)\n",
    "                # print(loss_mask.sum())\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                pred, h = model(x, input_mask, h.detach() if h is not None else None)\n",
    "\n",
    "                loss = criterion(pred, target)\n",
    "                masked_loss = (loss * loss_mask.unsqueeze(0)).sum() / loss_mask.sum()\n",
    "\n",
    "                masked_loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "                optimizer.step()\n",
    "\n",
    "                total_loss += masked_loss.item()\n",
    "                count += 1\n",
    "\n",
    "        if (epoch + 1) % 5 == 0:\n",
    "            print(f\"Epoch {epoch+1}/{epochs}, Loss: {total_loss/count:.6f}\")\n",
    "\n",
    "    return model"
   ],
   "id": "5b7f2136100fdf7a",
   "outputs": [],
   "execution_count": 211
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-29T15:21:12.361757Z",
     "start_time": "2025-10-29T15:21:08.723845Z"
    }
   },
   "cell_type": "code",
   "source": [
    "np.random.seed(10)\n",
    "\n",
    "# Train/test = 80/20\n",
    "n_train = int(len(data) * 0.8)\n",
    "\n",
    "turb_1 = data.values.astype('float32').reshape(1, -1, 1)\n",
    "train_data = turb_1[:, :n_train, :]\n",
    "test_data = turb_1[:, n_train:, :]\n",
    "\n",
    "train_loss_mask = ~np.isnan(train_data)\n",
    "test_loss_mask = ~np.isnan(test_data)\n",
    "\n",
    "train_input_mask = (np.random.random(train_data.shape) < 0.1) | np.isnan(train_data)\n",
    "test_input_mask = (np.random.random(test_data.shape) < 0.1) | np.isnan(test_data)\n",
    "\n",
    "# print(f\"Training on {train_data.shape} with {np.isnan(train_data).mean()*100:.1f}% NaNs\")\n",
    "\n",
    "model = train_baseline(torch.tensor(train_data), torch.tensor(train_input_mask), torch.tensor(train_loss_mask), epochs=20, chunk_size=500)"
   ],
   "id": "e2726b007ebad198",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/20, Loss: 0.180477\n",
      "Epoch 10/20, Loss: 0.178619\n",
      "Epoch 15/20, Loss: 0.177926\n",
      "Epoch 20/20, Loss: 0.177549\n"
     ]
    }
   ],
   "execution_count": 212
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-29T15:22:33.839002Z",
     "start_time": "2025-10-29T15:22:33.776608Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def test_baseline(model, data, test_input_mask, test_loss_mask, chunk_size=500):\n",
    "    device = next(model.parameters()).device\n",
    "    criterion = nn.MSELoss(reduction='none')\n",
    "\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    count = 0\n",
    "    all_predictions = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        n_samples, n_timesteps, n_features = data.shape\n",
    "\n",
    "        for i in range(n_samples):\n",
    "            h = None\n",
    "            preds = []\n",
    "\n",
    "            for start in range(0, n_timesteps, chunk_size):\n",
    "                end = min(start + chunk_size, n_timesteps)\n",
    "\n",
    "                # Extract chunk and masks\n",
    "                chunk = data[i, start:end].clone().to(device)  # [chunk_len, n_features]\n",
    "                input_mask = test_input_mask[i, start:end].to(device)  # [chunk_len, n_features]\n",
    "                loss_mask = test_loss_mask[i, start:end-1].to(device)  # [chunk_len - 1, n_features]\n",
    "\n",
    "                # Mask missing inputs\n",
    "                chunk[input_mask] = -1.0\n",
    "\n",
    "                # Input/output pairs\n",
    "                x = chunk[:-1].unsqueeze(0)      # [1, T-1, F]\n",
    "                target = chunk[1:].unsqueeze(0)  # [1, T-1, F]\n",
    "                input_mask = input_mask[:-1].unsqueeze(0)  # [1, T-1, F]\n",
    "                loss_mask = loss_mask.unsqueeze(0)\n",
    "\n",
    "                # Forward pass\n",
    "                pred, h = model(x, input_mask, h)\n",
    "\n",
    "                # Compute masked loss\n",
    "                loss = criterion(pred, target)\n",
    "                masked_loss = (loss * loss_mask).sum() / loss_mask.sum()\n",
    "                total_loss += masked_loss.item()\n",
    "                count += 1\n",
    "\n",
    "                preds.append(pred.squeeze(0).cpu())\n",
    "\n",
    "            all_predictions.append(torch.cat(preds, dim=0))  # [n_timesteps-1, n_features]\n",
    "\n",
    "    avg_loss = total_loss / count\n",
    "    all_predictions = torch.stack(all_predictions)  # [n_samples, n_timesteps-1, n_features]\n",
    "    return avg_loss, all_predictions\n",
    "\n",
    "test_baseline(model, torch.tensor(test_data), torch.tensor(test_input_mask), torch.tensor(test _loss_mask))"
   ],
   "id": "614fa27e50e8fb7d",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.17652383993069332,\n",
       " tensor([[[0.2851],\n",
       "          [0.4231],\n",
       "          [0.4233],\n",
       "          ...,\n",
       "          [0.0543],\n",
       "          [0.0346],\n",
       "          [0.0041]]]))"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 214
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "e8c92a8048ac7642"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
